{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e4bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16e6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8141dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import ActivationFunction\n",
    "from loss_function import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86214d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    A simple artificial neuron that computes a weighted sum of its inputs, applies an activation function, and produces an output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, activation ='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the neuron with random weights and bias.\n",
    "        \"\"\"\n",
    "\n",
    "        # xavier initialization for weights\n",
    "        limit = 1/math.sqrt(num_inputs)\n",
    "        self.weights = np.random.uniform(-limit,limit,num_inputs)\n",
    "\n",
    "        # Bias initialization\n",
    "        self.bias = np.random.uniform(-limit, limit)\n",
    "\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "\n",
    "        self.activation = ActivationFunction.get_activation(activation)\n",
    "        self.activation_derivative = ActivationFunction.get_activation_derivative(activation)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute output of the neurons in the layer given the inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        # preserve copy of original inputs \n",
    "        self.inputs = np.array(inputs)\n",
    "\n",
    "        # compute weighted sum\n",
    "        weighted_sum = np.dot(self.weights, self.inputs) + self.bias\n",
    "\n",
    "        # applying ReLU by default\n",
    "        self.output = self.activation(weighted_sum)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the neuron\"\"\"\n",
    "        return f\"Neuron(weights={[round(w, 3) for w in self.weights]}, bias={round(self.bias, 3)})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f8980e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A layer of neurons in a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_neurons, num_inputs_per_neuron=None, activation='relu', is_output=False):\n",
    "        \"\"\"\n",
    "        Initialize the layer with given number of neurons, each with specified number of inputs.\n",
    "\n",
    "        Args: \n",
    "            num_neurons (int): Number of neurons in the layer.\n",
    "            num_inputs_per_neuron (int): Number of inputs each neuron receives.\n",
    "            is_output (bool): Flag indicating if this layer is the output layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_inputs_per_neuron = num_inputs_per_neuron\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # creating neurons for the layer\n",
    "        self.neurons = [Neuron(num_inputs_per_neuron, activation) for _ in range(num_neurons)]\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forwards pass through layers sequentially by computing outputs of all neurons in the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs = np.array(inputs)\n",
    "\n",
    "        # get output from each neuron\n",
    "        self.outputs = np.array([neuron.forward(inputs) for neuron in self.neurons])\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the layer\"\"\"\n",
    "        layer_type = \"Output\" if self.is_output else \"Hidden\"\n",
    "        return f\"{layer_type} Layer ({self.num_neurons} neurons, {self.num_inputs_per_neuron} inputs each)\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "        Neural Network consisting of multiple layers.\n",
    "        Basic idea of Multi-Layer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_name='mse'):\n",
    "\n",
    "\n",
    "        self. layers = []\n",
    "        self.loss_fn = LossFunction.get_loss_function(loss_name)\n",
    "\n",
    "    def add_layer(self, num_neurons, num_inputs=None, activation = 'relu',is_output=False):\n",
    "        \"\"\"\n",
    "        Initialize layers and add to the network.\n",
    "\n",
    "        Args:\n",
    "            num_neurons (int): Number of neurons in the layer.\n",
    "            num_inputs (int): Number of inputs each neuron receives. Required for the first layer.\n",
    "            activation (str): Activation function to be used in the layer.\n",
    "            is_output (bool): Flag indicating if this layer is the output layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.layers and num_inputs is None:\n",
    "            raise ValueError(\"Number of inputs must be specified for the first layer.\")\n",
    "        \n",
    "        num_inputs_per_neuron = num_inputs if not self.layers else self.layers[-1].num_neurons  # get from previous layer\n",
    "        layer = Layer(num_neurons, num_inputs_per_neuron, activation, is_output)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire network.\n",
    "        \"\"\"\n",
    "\n",
    "        current_input = np.array(inputs)\n",
    "        for layer in self.layers: \n",
    "            current_input = layer.forward(current_input)\n",
    "        return current_input\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict outputs for given inputs X.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self.forward(x) for x in X])\n",
    "        \n",
    "    \n",
    "    # fit method for training \n",
    "    def fit(self, X,y, epochs =100, learning_rate =0.01):\n",
    "        \"\"\"\n",
    "        Train the network using Gradient descent\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for inputs, targets in zip(X,y):\n",
    "                \n",
    "                # forward pass\n",
    "                predictions = self.predict(inputs)\n",
    "\n",
    "                # compute loss \n",
    "                loss = self.loss_fn(predictions, targets)\n",
    "                total_loss += loss\n",
    "\n",
    "                # backward pass \n",
    "                # self.backward(predictions, targets)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss/len(X)}\")\n",
    "\n",
    "        \n",
    "        # final outcome \n",
    "        print(f\"Final Loss: {total_loss/len(X)}\")\n",
    "\n",
    "    \n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the network\"\"\"\n",
    "        return f\"Network(layers={self.num_layers}, layer_sizes={self.layer_sizes})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb8ca4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Architecture:\n",
      "Layer 1: Hidden Layer (2 neurons, 2 inputs each)\n",
      "  Neuron 1: Neuron(weights=[-0.177, 0.637], bias=0.328)\n",
      "  Neuron 2: Neuron(weights=[0.14, -0.486], bias=-0.486)\n",
      "Layer 2: Hidden Layer (3 neurons, 2 inputs each)\n",
      "  Neuron 1: Neuron(weights=[-0.625, 0.518], bias=0.143)\n",
      "  Neuron 2: Neuron(weights=[0.294, -0.678], bias=0.665)\n",
      "  Neuron 3: Neuron(weights=[0.47, -0.407], bias=-0.45)\n",
      "Layer 3: Output Layer (1 neurons, 3 inputs each)\n",
      "  Neuron 1: Neuron(weights=[-0.366, -0.226, 0.029], bias=-0.079)\n"
     ]
    }
   ],
   "source": [
    "nw = Network(loss_name='huber')\n",
    "\n",
    "nw.add_layer(num_neurons=2, num_inputs=2, activation='relu') \n",
    "nw.add_layer(num_neurons=3, activation='tanh')\n",
    "nw.add_layer(num_neurons=1, activation='linear', is_output=True)\n",
    "\n",
    "\n",
    "print(\"Network Architecture:\")\n",
    "for i, layer in enumerate(nw.layers):\n",
    "    print(f\"Layer {i+1}: {layer}\")\n",
    "    for j, neuron in enumerate(layer.neurons):\n",
    "        print(f\"  Neuron {j+1}: {neuron}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e95bda5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output: [[[-0.22699173 -0.1461266 ]]\n",
      "\n",
      " [[-0.15629815 -0.29327881]]]\n",
      "\n",
      "Layer by layer:\n",
      "Layer 1 output: [0.         0.31295952]\n",
      "Layer 2 (final) output: [ 0.29594065  0.42384324 -0.52068839]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([0.5, -1.5])\n",
    "output = nw.predict(input_data)\n",
    "print(f\"Network output: {output}\")\n",
    "\n",
    "# You can also test layer by layer:\n",
    "print(\"\\nLayer by layer:\")\n",
    "layer1_output = nw.layers[0].forward(input_data)\n",
    "print(f\"Layer 1 output: {layer1_output}\")\n",
    "\n",
    "layer2_output = nw.layers[1].forward(layer1_output)\n",
    "print(f\"Layer 2 (final) output: {layer2_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa76f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4366548085687097\n",
      "Epoch 10, Loss: 0.4366548085687097\n",
      "Epoch 20, Loss: 0.4366548085687097\n",
      "Epoch 30, Loss: 0.4366548085687097\n",
      "Epoch 40, Loss: 0.4366548085687097\n",
      "Final Loss: 0.4366548085687097\n"
     ]
    }
   ],
   "source": [
    "X_train = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n",
    "y_train = [[0.3], [0.7], [1.1]]\n",
    "\n",
    "nw.fit(X_train, y_train, epochs=50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428dba3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16543196],\n",
       "       [-0.15094246]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw.predict([[0.2, 0.4],[0.5,0.6]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
