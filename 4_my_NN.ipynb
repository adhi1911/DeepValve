{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "93e4bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d16e6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8141dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import ActivationFunction\n",
    "from loss_function import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "86214d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    A simple artificial neuron that computes a weighted sum of its inputs, applies an activation function, and produces an output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, activation ='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the neuron with random weights and bias.\n",
    "        \"\"\"\n",
    "\n",
    "        # xavier initialization for weights\n",
    "        limit = 1/math.sqrt(num_inputs)\n",
    "        self.weights = np.random.uniform(-limit,limit,num_inputs)\n",
    "\n",
    "        # Bias initialization\n",
    "        self.bias = np.random.uniform(-limit, limit)\n",
    "\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "\n",
    "        self.activation = ActivationFunction.get_activation(activation)\n",
    "        self.activation_derivative = ActivationFunction.get_activation_derivative(activation)\n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute output of the neurons in the layer given the inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        # preserve copy of original inputs \n",
    "        self.inputs = np.array(inputs)\n",
    "        # compute weighted sum\n",
    "        weighted_sum = np.dot(self.weights, self.inputs) + self.bias\n",
    "\n",
    "        # applying ReLU by default\n",
    "        self.output = self.activation(weighted_sum)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def update_weights(self, dl_dw, dl_db, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights and bias using gradients and learning rate.\n",
    "        \"\"\"\n",
    "        # Ensure dl_dw is a NumPy array\n",
    "        dl_dw = np.array(dl_dw)\n",
    "\n",
    "        # Update weights\n",
    "        self.weights -= learning_rate * dl_dw\n",
    "\n",
    "        # Update bias\n",
    "        self.bias -= learning_rate * dl_db\n",
    "\n",
    "    \n",
    "    def backward(self, dL_dy, lenning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients and update weights.\n",
    "        Args:\n",
    "            dL_dy: Gradient of loss with respect to the neuron's output.\n",
    "            learning_rate: Learning rate for weight updates.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            dL_dx: Gradient of loss with respect to the neuron's inputs.\n",
    "\n",
    "        Info:\n",
    "        Using chain rule:\n",
    "        dL_dx = dL_dy * dy_dz * dz_dx\n",
    "\n",
    "        where:\n",
    "        - dL_dy: Gradient of loss with respect to the neuron's output (from next layer) \n",
    "        - dy_dz: Derivative of activation function at weighted sum\n",
    "        - dz_dx: Weights of the neuron\n",
    "\n",
    "        Gradients for weights and bias:\n",
    "        - dl_dw = dL_dz * inputs\n",
    "        - dl_db = dL_dz * 1\n",
    "        \"\"\"\n",
    "\n",
    "        dy_dz = self.activation_derivative(self.output)\n",
    "        dL_dz = dL_dy * dy_dz  \n",
    "\n",
    "        dl_dw = dL_dz * self.inputs  # gradient for weights\n",
    "        dl_dw = dl_dw\n",
    "        dl_db = dL_dz * 1 # gradient for bias\n",
    "\n",
    "        self.update_weights(dl_dw, dl_db, lenning_rate)\n",
    "\n",
    "        dL_dx = dL_dz * self.weights  # gradient with respect to inputs to pass to previous layer\n",
    "\n",
    "        return dL_dx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the neuron\"\"\"\n",
    "        return f\"Neuron(weights={[round(w, 3) for w in self.weights]}, bias={round(self.bias, 3)})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f7a3bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights: [-0.14486859  0.52044005  0.26788353]\n",
      "Initial Bias: 0.11392100481799905\n",
      "Shape of Weights: (3,)\n",
      "Shape of Bias: ()\n",
      "Output: 0.2838872148932628\n",
      "Output: 0.4769237117328132\n",
      "Output: 0.6699602085723636\n",
      "Outputs: 0.6699602085723636\n",
      "Updated Weights: [-0.14556859  0.51884005  0.26518353]\n",
      "Updated Bias: [0.112921 0.111921 0.110921]\n"
     ]
    }
   ],
   "source": [
    "# Create a neuron with 3 inputs\n",
    "neuron = Neuron(num_inputs=3, activation='relu')\n",
    "\n",
    "# Print initial weights and bias\n",
    "print(\"Initial Weights:\", neuron.weights)\n",
    "print(\"Initial Bias:\", neuron.bias)\n",
    "\n",
    "print(\"Shape of Weights:\", neuron.weights.shape)\n",
    "print(\"Shape of Bias:\", np.array(neuron.bias).shape)\n",
    "\n",
    "# Example batch of inputs and gradients\n",
    "inputs = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])  # Batch of 3 input vectors\n",
    "dL_dy = np.array([0.1, 0.2, 0.3])  # Gradients for each input vector\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Perform forward pass\n",
    "for single_input in inputs:\n",
    "    output = neuron.forward(single_input)\n",
    "    print(\"Output:\", output)\n",
    "print(\"Outputs:\", output)\n",
    "\n",
    "# Perform backward pass\n",
    "neuron.backward(dL_dy, learning_rate)\n",
    "\n",
    "# Print updated weights and bias\n",
    "print(\"Updated Weights:\", neuron.weights)\n",
    "print(\"Updated Bias:\", neuron.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9f8980e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A layer of neurons in a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_neurons, num_inputs_per_neuron=None, activation='relu', is_output=False):\n",
    "        \"\"\"\n",
    "        Initialize the layer with given number of neurons, each with specified number of inputs.\n",
    "\n",
    "        Args: \n",
    "            num_neurons (int): Number of neurons in the layer.\n",
    "            num_inputs_per_neuron (int): Number of inputs each neuron receives.\n",
    "            is_output (bool): Flag indicating if this layer is the output layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_inputs_per_neuron = num_inputs_per_neuron\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # creating neurons for the layer\n",
    "        self.neurons = [Neuron(num_inputs_per_neuron, activation) for _ in range(num_neurons)]\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forwards pass through layers sequentially by computing outputs of all neurons in the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs = np.array(inputs)\n",
    "\n",
    "        # get output from each neuron\n",
    "        self.outputs = np.array([neuron.forward(inputs) for neuron in self.neurons])\n",
    "\n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, dL_dy, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients and update weights for all neurons in the layer.\n",
    "\n",
    "        dL_dy: Gradient of loss with respect to the layer's outputs.\n",
    "        dL_dx : Gradient of loss with respect to the layer's inputs.\n",
    "        Returns dL_dx to propagate to previous layer.\n",
    "        \"\"\"\n",
    "\n",
    "        dL_dy_current = np.zeros(self.num_inputs_per_neuron) # Gradient accumulator for inputs to this layer\n",
    "\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            dL_dx = neuron.backward(dL_dy[i], learning_rate)  # get gradient w.r.t inputs from each neuron\n",
    "            dL_dy_current += dL_dx # summation of gradients \n",
    "\n",
    "        return dL_dy_current   # propagate gradient to previous layer\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the layer\"\"\"\n",
    "        layer_type = \"Output\" if self.is_output else \"Hidden\"\n",
    "        return f\"{layer_type} Layer ({self.num_neurons} neurons, {self.num_inputs_per_neuron} inputs each)\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b02b9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "        Neural Network consisting of multiple layers.\n",
    "        Basic idea of Multi-Layer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_name='mse'):\n",
    "\n",
    "\n",
    "        self. layers = []\n",
    "        self.loss_fn = LossFunction.get_loss_function(loss_name)\n",
    "        self.loss_fn_derivative = LossFunction.get_loss_derivative(loss_name)\n",
    "\n",
    "    def add_layer(self, num_neurons, num_inputs=None, activation = 'relu',is_output=False):\n",
    "        \"\"\"\n",
    "        Initialize layers and add to the network.\n",
    "\n",
    "        Args:\n",
    "            num_neurons (int): Number of neurons in the layer.\n",
    "            num_inputs (int): Number of inputs each neuron receives. Required for the first layer.\n",
    "            activation (str): Activation function to be used in the layer.\n",
    "            is_output (bool): Flag indicating if this layer is the output layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.layers and num_inputs is None:\n",
    "            raise ValueError(\"Number of inputs must be specified for the first layer.\")\n",
    "        \n",
    "        num_inputs_per_neuron = num_inputs if not self.layers else self.layers[-1].num_neurons  # get from previous layer\n",
    "        layer = Layer(num_neurons, num_inputs_per_neuron, activation, is_output)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire network.\n",
    "        \"\"\"\n",
    "\n",
    "        current_input = np.array(inputs)\n",
    "        for layer in self.layers: \n",
    "            current_input = layer.forward(current_input)\n",
    "        return current_input\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict outputs for given inputs X.\"\"\"\n",
    "        X = np.array(X)\n",
    "        if X.ndim == 1:\n",
    "            # Single input vector\n",
    "            return self.forward(X)\n",
    "        else:\n",
    "            # Batch of input vectors\n",
    "            return np.array([self.forward(x) for x in X])\n",
    "    \n",
    "    def backward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients and update weights.\n",
    "        dL_dy: Gradient of loss with respect to the network's output.\n",
    "        \"\"\"\n",
    "\n",
    "        dL_dy = self.loss_fn_derivative(predictions, targets)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            dL_dy = layer.backward(dL_dy, self.learning_rate)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # fit method for training \n",
    "    def fit(self, X,y, epochs =100, learning_rate =0.01):\n",
    "        \"\"\"\n",
    "        Train the network using Gradient descent\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input data.\n",
    "            y (array-like): Target labels.\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for weight updates.\n",
    "\n",
    "        Info:\n",
    "            For each epoch, perform forward pass, compute loss, and backward pass to update weights.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for inputs, targets in zip(X,y):\n",
    "                targets = np.array(targets).reshape(-1) \n",
    "                # forward pass\n",
    "                predictions = self.predict(inputs)\n",
    "\n",
    "                # compute loss \n",
    "                loss = self.loss_fn(predictions, targets)\n",
    "                total_loss += loss\n",
    "\n",
    "                # backward pass \n",
    "                self.backward(predictions, targets)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss/len(X)}\")\n",
    "\n",
    "        \n",
    "        # final outcome \n",
    "        print(f\"Final Loss: {total_loss/len(X)}\")\n",
    "\n",
    "    \n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the network\"\"\"\n",
    "        return f\"Network(layers={self.num_layers}, layer_sizes={self.layer_sizes})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bb8ca4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Architecture:\n",
      "Layer 1: Hidden Layer (2 neurons, 2 inputs each)\n",
      "  Neuron 1: Neuron(weights=[-0.189, -0.062], bias=0.403)\n",
      "  Neuron 2: Neuron(weights=[-0.425, 0.02], bias=0.131)\n",
      "Layer 2: Hidden Layer (3 neurons, 2 inputs each)\n",
      "  Neuron 1: Neuron(weights=[-0.641, 0.152], bias=-0.466)\n",
      "  Neuron 2: Neuron(weights=[-0.615, 0.635], bias=0.659)\n",
      "  Neuron 3: Neuron(weights=[0.436, -0.276], bias=-0.569)\n",
      "Layer 3: Output Layer (1 neurons, 3 inputs each)\n",
      "  Neuron 1: Neuron(weights=[0.213, -0.069, -0.436], bias=-0.006)\n"
     ]
    }
   ],
   "source": [
    "nw = Network(loss_name='huber')\n",
    "\n",
    "nw.add_layer(num_neurons=2, num_inputs=2, activation='relu') \n",
    "nw.add_layer(num_neurons=3, activation='tanh')\n",
    "nw.add_layer(num_neurons=1, activation='linear', is_output=True)\n",
    "\n",
    "\n",
    "print(\"Network Architecture:\")\n",
    "for i, layer in enumerate(nw.layers):\n",
    "    print(f\"Layer {i+1}: {layer}\")\n",
    "    for j, neuron in enumerate(layer.neurons):\n",
    "        print(f\"  Neuron {j+1}: {neuron}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e95bda5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output: [-0.00077757]\n",
      "\n",
      "Layer by layer:\n",
      "Layer 1 output: [0.4019929 0.       ]\n",
      "Layer 2 (final) output: [-0.61925385  0.38951937 -0.37450472]\n",
      "Layer 1, Neuron 1 - Weights shape: (2,), Bias shape: ()\n",
      "Layer 1, Neuron 2 - Weights shape: (2,), Bias shape: ()\n",
      "Layer 2, Neuron 1 - Weights shape: (2,), Bias shape: ()\n",
      "Layer 2, Neuron 2 - Weights shape: (2,), Bias shape: ()\n",
      "Layer 2, Neuron 3 - Weights shape: (2,), Bias shape: ()\n",
      "Layer 3, Neuron 1 - Weights shape: (3,), Bias shape: ()\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([0.5, -1.5])\n",
    "output = nw.predict(input_data)\n",
    "print(f\"Network output: {output}\")\n",
    "\n",
    "# You can also test layer by layer:\n",
    "print(\"\\nLayer by layer:\")\n",
    "layer1_output = nw.layers[0].forward(input_data)\n",
    "print(f\"Layer 1 output: {layer1_output}\")\n",
    "\n",
    "layer2_output = nw.layers[1].forward(layer1_output)\n",
    "print(f\"Layer 2 (final) output: {layer2_output}\")\n",
    "\n",
    "\n",
    "# shape of neurons weight and bias \n",
    "for i, layer in enumerate(nw.layers):\n",
    "    for j, neuron in enumerate(layer.neurons):\n",
    "        print(f\"Layer {i+1}, Neuron {j+1} - Weights shape: {neuron.weights.shape}, Bias shape: {np.array(neuron.bias).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "fa76f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.13513185374227343\n",
      "Epoch 10, Loss: 0.07190008716524127\n",
      "Epoch 20, Loss: 0.04317580745705396\n",
      "Epoch 30, Loss: 0.03029933226451267\n",
      "Epoch 40, Loss: 0.02459714894995591\n",
      "Final Loss: 0.022263918997647927\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "y_train = np.array([0.3, 0.7])\n",
    "\n",
    "nw.fit(X_train, y_train, epochs=50, learning_rate=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "428dba3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44004726],\n",
       "       [0.445667  ]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw.predict([[0.2, 0.4],[0.5,0.6]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
